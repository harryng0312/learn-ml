{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# my project\n",
    "from modules.conf import PROJECT_DIR\n",
    "\n",
    "# %matplotlib tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = np.loadtxt(fname=\"\".join([PROJECT_DIR, \"/data/regression/winequality-red.csv\"]), delimiter=\",\", dtype=float, skiprows=1)\n",
    "    # return data[:,0].reshape(data.shape[0], 1), [map_label(i) for i in data[:, -1]]\n",
    "    return data[:,:-1], [0 if i < 6 else 1 for i in data[:, -1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define sigmoid:\n",
    "- $z = \\mathbf w \\cdot \\mathbf x + b$\n",
    "- $f(z) = sigmoid(z) = f_{w,b}(x) = sigmoid(\\mathbf w \\cdot \\mathbf x + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> float:\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calculate Cost function\n",
    "$$\n",
    "J(w,b) = \\frac{1}{m}\\sum_{i=0}^{m-1}[(-y_i(f_{\\mathbf{\\mathbf w},b}( \\mathbf{x}_i)) - (1 - y_i)( 1 - f_{\\mathbf{\\mathbf w},b}( \\mathbf{x}_i))]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cost(w: np.ndarray, b: float, X: np.ndarray, y: np.ndarray) -> float:\n",
    "    m, n = X.shape\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        z = np.dot(X[i],w) + b\n",
    "        f_wb = sigmoid(z)\n",
    "        cost += -y[i]*np.log(f_wb) - (1-y[i])*np.log(1-f_wb)\n",
    "    cost /= m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate gradient\n",
    "$$\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient(w: np.ndarray, b: float, X: np.ndarray, y: np.ndarray, lambda_: float= 1.0) -> tuple:\n",
    "    \"\"\"\n",
    "    w: (n,1)\n",
    "    b: scalar\n",
    "    X: (m, n) - m: count of rows, n: number of features\n",
    "    y: (m,1) - label\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    w_result = np.zeros(shape=w.shape)\n",
    "    b_result = 0.0\n",
    "    for i in range(m):\n",
    "        zi = np.dot(w, X[i]) + b\n",
    "        f = sigmoid(z=zi)\n",
    "        f_y = (f - y[i])\n",
    "        for j in range(n):\n",
    "            w_result[j] += f_y * X[i, j]\n",
    "            pass\n",
    "        b_result += f_y\n",
    "        pass\n",
    "    \n",
    "    for i in range(n):\n",
    "        # w_result[i] += lambda_ * w[i]\n",
    "        w_result[i] += lambda_ * w[i]\n",
    "        \n",
    "    w_result /= m\n",
    "    b_result /= m\n",
    "    return w_result, b_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Features scaling *(optional)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "# scaler = StandardScaler()\n",
    "def features_scalling(X: np.ndarray) -> np.ndarray:\n",
    "    # z-scored\n",
    "    # m = X.shape[0]\n",
    "    # n = X.shape[1]\n",
    "\n",
    "    result = scaler.fit(X=X)\n",
    "    return result.transform(X=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Calculate gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_descent(X: np.ndarray, y: np.ndarray, w_0: np.ndarray, b_0: float, grad_func: calc_gradient, alpha: float, iter: int = 1000) -> tuple:\n",
    "    w = [w_0]\n",
    "    b = [b_0]\n",
    "    min_cost = -1\n",
    "    for _ in range(iter):\n",
    "        grad_w, grad_b = grad_func(w[-1], b[-1], X, y)\n",
    "        w_new = w[-1] - alpha * grad_w\n",
    "        b_new = b[-1] - alpha * grad_b\n",
    "        # print(f\"grad_w:{grad_w} grad_b:{grad_b} |{np.linalg.norm(grad_w)}|{np.linalg.norm(grad_b)}\")\n",
    "        # print(f\"grad_b:{grad_b}\")\n",
    "        w.append(w_new)\n",
    "        b.append(b_new)\n",
    "        # if len(w) > 1  and np.linalg.norm(w[-1] - w[-2])< 1e-2:\n",
    "        #     break\n",
    "        cost = calc_cost(w[-1], b[-1], X, y)\n",
    "        # min_cost = cost if cost > min_cost else min_cost\n",
    "        if cost < 1e-2:\n",
    "            break\n",
    "        pass\n",
    "    return w, b, min_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w: np.ndarray, b: float, x: np.ndarray) -> float:\n",
    "    y_pred = sigmoid(z=np.dot(w, x) + b)\n",
    "    # print(f\"===== pred:{y_pred}\\nw={w}\\nb={b}\\nx={x}\")\n",
    "    return 0 if y_pred < 0.5 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_data()\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_train = features_scalling(X_train)\n",
    "# print(f\"X_train:{features_scalling(X_train)}\")\n",
    "# x: (n,)\n",
    "# y: (n,)\n",
    "# initialize fitting parameters. Recall that the shape of w is (n,)\n",
    "w_init = np.random.randn(X_train.shape[1])\n",
    "b_init = .3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"{w_init}\")\n",
    "# some gradient descent settings \n",
    "# iterations = 10_000\n",
    "# alpha = 1e-2\n",
    "iterations = 1_000\n",
    "alpha = 1\n",
    "w,b, min_cost = calc_gradient_descent(X_train ,y_train, w_init, b_init, calc_gradient, alpha, iterations)\n",
    "print(f\"w,b found by gradient descent:{w[-1]} {b[-1]} in {len(w)-1} iter with alpha:{alpha} and min cost:{min_cost}\")\n",
    "\n",
    "# w,b found by gradient descent:[-0.67651481  0.44027971  1.15695973  0.10045475  0.933882    0.16064972\n",
    "#  -0.23473286  0.59200058 -0.41696827  0.98328881 -0.69908097  2.60776716] 0.5916134283397572 in 1500 iter with alpha:0.01\n",
    "# w,b found by gradient descent:[-0.92221174 -0.37089413  1.52195306 -0.03876746  0.81788073  0.06667964\n",
    "#  -0.1696974   0.0168963  -2.42217425  0.96192688 -1.19904577  5.41492177] 0.015328354210714093 in 5000 iter with alpha:0.01\n",
    "\n",
    "# w,b found by gradient descent:[-0.28881403  0.87499792  0.98205555  0.54459359  0.98221817  0.02751566\n",
    "#  -0.03702636  0.83542444  0.447363    0.91674234 -0.45465462  0.48588827] 0.8351016088978512 in 1500 iter with alpha:0.001\n",
    "# w,b found by gradient descent:[-0.34706758  0.7416551   1.03519848  0.1860643   0.96395259  0.02103931\n",
    "#  -0.03100939  0.75921511  0.16963548  0.91385989 -0.49060692  0.96621209] 0.7590381904740958 in 5000 iter with alpha:0.001\n",
    "\n",
    "# w,b found by gradient descent:[ 0.62749218  0.97274034  0.99013865  0.89022076  0.99583291  0.40402159\n",
    "#  -0.59559367  0.95504917  0.8503904   0.97307271  0.55478787  0.7895114 ] 0.9549220973171965 in 1500 iter with alpha:0.0001\n",
    "# w,b found by gradient descent:[ 0.16331801  0.93680634  0.98079621  0.76836106  0.99085714  0.17218414\n",
    "#  -0.29055037  0.90057759  0.66849293  0.94350969  0.02677779  0.5705947 ] 0.900312046353353 in 5000 iter with alpha:0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with [[0.14148761]\n",
      " [0.00579496]\n",
      " [0.00961057]\n",
      " [0.08425345]\n",
      " [0.        ]\n",
      " [0.42765841]\n",
      " [1.        ]\n",
      " [0.02214223]\n",
      " [0.07924546]\n",
      " [0.01414161]\n",
      " [0.26072544]], the class is:0\n"
     ]
    }
   ],
   "source": [
    "x_test = np.asarray([6.0,0.31,0.47,3.6,0.067,18.0,42.0,0.99549,3.39,0.66,11.0])\n",
    "x_test = features_scalling(x_test.reshape(-1, 1))\n",
    "print(f\"with {x_test}, the class is:{predict(w[-1], b[-1], x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy:\n",
    "- 1400 for train, 199 for test: 0.4623115577889447\n",
    "- train, test are same data with reg: 0.5347091932457786, f1_score: 0.6968215158924205 in 10000 iter with alpha:0.006 and min cost:0.9534080672312636\n",
    "- train, test are same data without reg: accuracy=0.6153846153846154, f1_score=0.6483704974271012 in 10000 iter with alpha:0.01 and min cost:0.7700646348893576\n",
    "- train, test are same data without reg: accuracy=0.6172607879924953, f1_score=0.6370106761565836 in 10000 iter with alpha:0.006 and min cost:1.0829094643371437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.5747342088805504\n",
      "f1_score=0.7084048027444254\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_data()\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "y_pred = np.asarray([predict(w[-1], b[-1], x) for x in X_train])\n",
    "# y_pred = features_scalling(y_pred.reshape(-1, 1))\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_train, y_pred=np.round(y_pred), normalize=True)\n",
    "print(f\"accuracy={accuracy}\")\n",
    "\n",
    "f1_s = f1_score(y_true=y_train, y_pred=np.round(y_pred))\n",
    "print(f\"f1_score={f1_s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:[[ 0.02107605 -3.14253873 -0.68332061  0.03180987 -2.8557886   0.02381734\n",
      "  -0.0178897  -2.17758154 -0.7699977   2.26458814  0.78008801]] b:[-2.39219237]\n"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", tol=1e-2, max_iter=10_000)\n",
    "lg.fit(X=X_train, y=y_train)\n",
    "print(f\"w:{lg.coef_} b:{lg.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.7417135709818636\n",
      "f1_score=0.7572016460905351\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_data()\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "y_pred = lg.predict(X_train)\n",
    "# y_pred = features_scalling(y_pred.reshape(-1, 1))\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_train, y_pred=y_pred, normalize=True)\n",
    "print(f\"accuracy={accuracy}\")\n",
    "\n",
    "f1_s = f1_score(y_true=y_train, y_pred=y_pred)\n",
    "print(f\"f1_score={f1_s}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
