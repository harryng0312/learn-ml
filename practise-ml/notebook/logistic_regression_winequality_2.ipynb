{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.animation as animation\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# my project\n",
    "from modules.conf import PROJECT_DIR\n",
    "\n",
    "# %matplotlib tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = np.loadtxt(fname=\"\".join([PROJECT_DIR, \"/data/regression/winequality-red.csv\"]), delimiter=\",\", dtype=float, skiprows=1)\n",
    "    # return data[:,0].reshape(data.shape[0], 1), [map_label(i) for i in data[:, -1]]\n",
    "    return data[:,:-1], [0 if i < 6.5 else 1 for i in data[:, -1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define sigmoid:\n",
    "- $z = \\mathbf w \\cdot \\mathbf x + b$\n",
    "- $f(z) = sigmoid(z) = f_{w,b}(x) = sigmoid(\\mathbf w \\cdot \\mathbf x + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> float:\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calculate Cost function\n",
    "$$\n",
    "J(w,b) = \\frac{1}{m}\\sum_{i=0}^{m-1}[(-y_i(f_{\\mathbf{\\mathbf w},b}( \\mathbf{x}_i)) - (1 - y_i)( 1 - f_{\\mathbf{\\mathbf w},b}( \\mathbf{x}_i))]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cost(w: np.ndarray, b: float, X: np.ndarray, y: np.ndarray) -> float:\n",
    "    m, n = X.shape\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        z = np.dot(X[i],w) + b\n",
    "        f_wb = sigmoid(z)\n",
    "        cost += -y[i]*np.log(f_wb) - (1-y[i])*np.log(1-f_wb)\n",
    "    cost /= m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate gradient\n",
    "$$\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient(w: np.ndarray, b: float, X: np.ndarray, y: np.ndarray, lambda_: float= 1.0) -> tuple:\n",
    "    \"\"\"\n",
    "    w: (n,1)\n",
    "    b: scalar\n",
    "    X: (m, n) - m: count of rows, n: number of features\n",
    "    y: (m,1) - label\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    w_result = np.zeros(shape=w.shape)\n",
    "    b_result = 0.0\n",
    "    for i in range(m):\n",
    "        zi = (np.dot(w, X[i]) + b)\n",
    "        f = sigmoid(z=zi)\n",
    "        f_y = (f - y[i])\n",
    "        for j in range(n):\n",
    "            w_result[j] += f_y * X[i, j]\n",
    "            pass\n",
    "        b_result += f_y\n",
    "        pass\n",
    "    \n",
    "    for i in range(n):\n",
    "        w_result[i] += lambda_ * w[i]\n",
    "        \n",
    "    w_result /= m\n",
    "    b_result /= m\n",
    "    return w_result, b_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Features scaling *(optional)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler((1,2,))\n",
    "# scaler = StandardScaler()\n",
    "def features_scalling(X: np.ndarray) -> np.ndarray:\n",
    "    # z-scored\n",
    "    # m = X.shape[0]\n",
    "    # n = X.shape[1]\n",
    "\n",
    "    global scaler_result \n",
    "    scaler_result = scaler.fit(X=X)\n",
    "    return scaler_result.transform(X=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Calculate gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_descent(X: np.ndarray, y: np.ndarray, w_0: np.ndarray, b_0: float, grad_func: calc_gradient, alpha: float, iter: int = 1000) -> tuple:\n",
    "    w = [w_0]\n",
    "    b = [b_0]\n",
    "    min_cost = -1\n",
    "    for _ in range(iter):\n",
    "        grad_w, grad_b = grad_func(w[-1], b[-1], X, y)\n",
    "        w_new = w[-1] - alpha * grad_w\n",
    "        b_new = b[-1] - alpha * grad_b\n",
    "        # print(f\"grad_w:{grad_w} grad_b:{grad_b} |{np.linalg.norm(grad_w)}|{np.linalg.norm(grad_b)}\")\n",
    "        # print(f\"grad_b:{grad_b}\")\n",
    "        w.append(w_new)\n",
    "        b.append(b_new)\n",
    "        # if np.linalg.norm(grad_w) < 3 and np.linalg.norm(grad_b) < 6e-1:\n",
    "        #     break\n",
    "        # cost = calc_cost(w[-1], b[1], X, y)\n",
    "        # min_cost = cost if cost > min_cost else min_cost\n",
    "        # if cost < 6e-1:\n",
    "        #     break\n",
    "        pass\n",
    "    return w, b, min_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w: np.ndarray, b: float, x: np.ndarray) -> float:\n",
    "    y_pred = sigmoid(z=np.dot(w, x) + b)\n",
    "    # print(f\"===== pred:{y_pred}\\nw={w}\\nb={b}\\nx={x}\")\n",
    "    return 0 if y_pred < 0.5 else 1\n",
    "    # return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_feature(X: np.ndarray) -> np.ndarray:\n",
    "    m, n  = X.shape\n",
    "    x_result = np.zeros(shape=(m,))\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            x_result[i] += X[i, j] * 2**j\n",
    "            pass\n",
    "        pass\n",
    "    return x_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_data()\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_train = features_scalling(X_train)\n",
    "X_train = map_feature(X_train).reshape(-1, 1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "res = scaler.fit(X_train)\n",
    "X_train = res.transform(X_train)\n",
    "\n",
    "# print(f\"X_train:{X_train}\")\n",
    "# x: (n,)\n",
    "# y: (n,)\n",
    "# initialize fitting parameters. Recall that the shape of w is (n,)\n",
    "w_init = np.random.randn(X_train.shape[1])\n",
    "b_init = .3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some gradient descent settings \n",
    "iterations = 1_000\n",
    "alpha = 1\n",
    "w,b, min_cost = calc_gradient_descent(X_train ,y_train, w_init, b_init, calc_gradient, alpha, iterations)\n",
    "print(f\"w,b found by gradient descent:{w[-1]} {b[-1]} in {len(w)-1} iter with alpha:{alpha} and min cost:{min_cost}\")\n",
    "\n",
    "# w,b found by gradient descent:[-0.67651481  0.44027971  1.15695973  0.10045475  0.933882    0.16064972\n",
    "#  -0.23473286  0.59200058 -0.41696827  0.98328881 -0.69908097  2.60776716] 0.5916134283397572 in 1500 iter with alpha:0.01\n",
    "# w,b found by gradient descent:[-0.92221174 -0.37089413  1.52195306 -0.03876746  0.81788073  0.06667964\n",
    "#  -0.1696974   0.0168963  -2.42217425  0.96192688 -1.19904577  5.41492177] 0.015328354210714093 in 5000 iter with alpha:0.01\n",
    "\n",
    "# w,b found by gradient descent:[-0.28881403  0.87499792  0.98205555  0.54459359  0.98221817  0.02751566\n",
    "#  -0.03702636  0.83542444  0.447363    0.91674234 -0.45465462  0.48588827] 0.8351016088978512 in 1500 iter with alpha:0.001\n",
    "# w,b found by gradient descent:[-0.34706758  0.7416551   1.03519848  0.1860643   0.96395259  0.02103931\n",
    "#  -0.03100939  0.75921511  0.16963548  0.91385989 -0.49060692  0.96621209] 0.7590381904740958 in 5000 iter with alpha:0.001\n",
    "\n",
    "# w,b found by gradient descent:[ 0.62749218  0.97274034  0.99013865  0.89022076  0.99583291  0.40402159\n",
    "#  -0.59559367  0.95504917  0.8503904   0.97307271  0.55478787  0.7895114 ] 0.9549220973171965 in 1500 iter with alpha:0.0001\n",
    "# w,b found by gradient descent:[ 0.16331801  0.93680634  0.98079621  0.76836106  0.99085714  0.17218414\n",
    "#  -0.29055037  0.90057759  0.66849293  0.94350969  0.02677779  0.5705947 ] 0.900312046353353 in 5000 iter with alpha:0.0001\n",
    "\n",
    "# w,b found by gradient descent:[3.16216087] -0.8470497215286827 in 5000 iter with alpha:0.03 and min cost:-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy:\n",
    "- 1400 for train, 199 for test: 0.4623115577889447\n",
    "- train, test are same data with reg: 0.5347091932457786, f1_score: 0.6968215158924205 in 10000 iter with alpha:0.006 and min cost:0.9534080672312636\n",
    "- train, test are same data without reg: accuracy=0.6153846153846154, f1_score=0.6483704974271012 in 10000 iter with alpha:0.01 and min cost:0.7700646348893576\n",
    "- train, test are same data with reg: accuracy=0.6172607879924953, f1_score=0.6370106761565836 in 10000 iter with alpha:0.006 and min cost:1.0829094643371437\n",
    "- train, test are same data compose map_feature with reg: accuracy=0.6985616010006254, f1_score=0.7229885057471265 in 5000 with iter with alpha:0.03 and min cost: -1\n",
    "- train, test are same data compose map_feature with reg: accuracy=0.699812382739212 f1_score=0.7125748502994012 in 1000 iter with alpha:1 and min cost:-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_scalling' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m X_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(X_train)\n\u001b[1;32m      3\u001b[0m y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(y_train)\n\u001b[0;32m----> 4\u001b[0m X_train \u001b[39m=\u001b[39m features_scalling(X_train)\n\u001b[1;32m      5\u001b[0m X_train \u001b[39m=\u001b[39m map_feature(X_train)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# scaler = MinMaxScaler()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_scalling' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_data()\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_train = features_scalling(X_train)\n",
    "X_train = map_feature(X_train).reshape(-1, 1)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "res = scaler.fit(X_train)\n",
    "X_train = res.transform(X_train)\n",
    "y_pred = np.asarray([predict(w[-1], b[-1], x) for x in X_train])\n",
    "# y_pred = features_scalling(y_pred.reshape(-1, 1))\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_train, y_pred=np.round(y_pred), normalize=True)\n",
    "print(f\"accuracy={accuracy}\")\n",
    "\n",
    "f1_s = f1_score(y_true=y_train, y_pred=np.round(y_pred))\n",
    "print(f\"f1_score={f1_s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.asarray([[6.0,0.31,0.47,3.6,0.067,18.0,42.0,0.99549,3.39,0.66,11.0]])\n",
    "# x_test = features_scalling(x_test)\n",
    "x_test = scaler_result.transform(x_test)\n",
    "x_test = map_feature(x_test).reshape(-1,1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "res = scaler.fit(X_train)\n",
    "X_train = res.transform(X_train)\n",
    "# x_test = scaler_result.transform(x_test)\n",
    "print(f\"with {x_test}, the class is:{predict(w[-1], b[-1], x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.arange(0, 1, 1e-3)\n",
    "y_pred = np.asarray([predict(w[-1], b[-1], x) for x in x_test ])\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "axes.plot(X_train, y_train, \"ro\")\n",
    "axes.plot(x_test, y_pred, \"b-\")\n",
    "\n",
    "axes.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check by sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:[[-0.03250397 -3.27387016  0.          0.11539285 -3.53908875  0.01340491\n",
      "  -0.01733205 -2.196248   -2.30382102  2.64324416  0.94147356]] b:[-1.78905337]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_data()\n",
    "# lg = LogisticRegression(tol=1e-4, max_iter=1_000)\n",
    "lg = LogisticRegression(penalty=\"l1\", solver=\"liblinear\") \n",
    "lg = lg.fit(X=X_train, y=y_train)\n",
    "\n",
    "print(f\"w:{lg.coef_} b:{lg.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.asarray([[6.0,0.31,0.47,3.6,0.067,18.0,42.0,0.99549,3.39,0.66,11.0]])\n",
    "# x_test = features_scalling(x_test)\n",
    "x_test = scaler_result.transform(x_test)\n",
    "x_test = map_feature(x_test).reshape(-1,1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "res = scaler.fit(X_train)\n",
    "X_train = res.transform(X_train)\n",
    "# x_test = scaler_result.transform(x_test)\n",
    "print(f\"with {x_test}, the class is:{lg.predict(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1599 1599\n",
      "1279 1279\n",
      "accuracy=0.86875\n",
      "f1_score=0.36363636363636365\n"
     ]
    }
   ],
   "source": [
    "X, y = load_data()\n",
    "scaler = StandardScaler()\n",
    "res = scaler.fit(X)\n",
    "X = res.transform(X)\n",
    "print(f\"{len(X)} {len(y)}\")\n",
    "from sklearn.model_selection  import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, np.asarray(y), test_size = 0.20 , random_state = 5)\n",
    "print(f\"{len(X_train)} {len(y_train)}\")\n",
    "# lg = LogisticRegression(penalty=\"l1\", solver=\"liblinear\") \n",
    "lg = LogisticRegression(max_iter=1000)\n",
    "lg = lg.fit(X=X_train, y=y_train)\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler = StandardScaler()\n",
    "# res = scaler.fit(X_train)\n",
    "# X_train = res.transform(X_train)\n",
    "# print(f\"x:{lg.predict(np.asarray([X_train[0]]))}\")\n",
    "y_pred = np.asarray([lg.predict([x]) for x in X_test])\n",
    "# y_pred = features_scalling(y_pred.reshape(-1, 1))\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_test, y_pred=y_pred, normalize=True)\n",
    "print(f\"accuracy={accuracy}\")\n",
    "\n",
    "f1_s = f1_score(y_true=y_test, y_pred=y_pred)\n",
    "print(f\"f1_score={f1_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.arange(0, 1, 1e-3)\n",
    "# y_pred = np.asarray([lg.predict(x.reshape(-1, 1)) for x in x_test ])\n",
    "y_pred = np.asarray([lg.predict(x.reshape(-1, 1)) for x in x_test ])\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "axes.plot(X_train, y_train, \"ro\")\n",
    "axes.plot(x_test, y_pred, \"b-\")\n",
    "\n",
    "# axes.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "x_min, x_max = (-.5, 1.5)\n",
    "x_test = np.arange(x_min, x_max, 1e-3)\n",
    "# y_pred = np.asarray([lg.predict(x.reshape(-1, 1)) for x in x_test ])\n",
    "y_pred_checked = np.asarray([lg.predict(x.reshape(-1, 1)) for x in x_test ])\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "axes.plot(X_train, y_train, \"ro\")\n",
    "axes.plot(x_test, y_pred_checked, \"g-\")\n",
    "y_pred_curve = np.asarray([sigmoid(w[-1]*x + b[-1]) for x in x_test ])\n",
    "y_pred_line = np.asarray([predict(w[-1], b[-1], x) for x in x_test ])\n",
    "curved = axes.plot(x_test, y_pred_curve, \"b--\")[0]\n",
    "line = axes.plot(x_test, y_pred_line, \"y-.\")[0]\n",
    "\n",
    "axes.axis([x_min, x_max, -0.5, 1.5])\n",
    "curved.set_label(\"Sigmoid curve\")\n",
    "line.set_label(\"\")\n",
    "axes.legend()\n",
    "def update_plot(i: int) -> None:\n",
    "    if i < 100:\n",
    "        time.sleep(0.09)\n",
    "    y_pred_curve = np.asarray([sigmoid(w[i]*x + b[i]) for x in x_test])\n",
    "    y_pred_line = np.asarray([predict(w[i], b[i], x) for x in x_test])\n",
    "    curved.set_ydata(y_pred_curve)\n",
    "    line.set_ydata(y_pred_line)\n",
    "    return\n",
    "ani1 = animation.FuncAnimation(fig=fig, func=update_plot, frames=len(w)-1, interval=10, repeat=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
